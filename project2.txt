# -*- coding: utf-8 -*-
"""project2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RsRjgU9hXKkstBVBAdgX7QBvoi2aq4VZ
"""

from google.colab import files
uploaded = files.upload()
import re
import os
import zipfile

local_zip = r"archive (2).zip"
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

for root,dirs,files in os.walk('/tmp'):
  for file in files:
    print(os.path.join(root,file))

import pandas as pd
import numpy as np
import seaborn as sns
import warnings
import matplotlib.pyplot as plt

dataset = pd.read_csv('/tmp/IMDB Dataset.csv')
print(dataset.columns)
print(dataset.iloc[1])

review = dataset['review']
sentiment = dataset['sentiment']
print(review[1])
print(sentiment[1])

print(dataset['sentiment'].nunique())

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer,WordNetLemmatizer

# Download NLTK data files
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_data(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Tokenize
    tokens = text.split()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    #Stemmer
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
    # Lemmatization
    #lemmatizer = WordNetLemmatizer()
    #lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]
    # Join tokens back
    preprocessed_text = ' '.join(stemmed_tokens)

    return preprocessed_text

sample = "The acting in this movie was top-notch, but the plot was weak!"
print(preprocess_data(sample))

preprocessed = dataset['review'].apply(preprocess_data)

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report , confusion_matrix
from sklearn.svm import LinearSVC

X_train, X_test, y_train, y_test = train_test_split(preprocessed, sentiment, test_size=0.2, random_state=42)
sentiment = sentiment.map({"positive": 1, "negative": 0})

vectorize = TfidfVectorizer(max_features=1000)
X_train_vectorized = vectorize.fit_transform(X_train)
X_test_vectorized = vectorize.transform(X_test)

model_1 = LogisticRegression()
model_1.fit(X_train_vectorized, y_train)
y_pred_1 = model_1.predict(X_test_vectorized)

print('Accuracy : ',accuracy_score(y_test,y_pred_1))
print('Classification Report\n',classification_report(y_test,y_pred_1))
print('Confusion Matrix\n',confusion_matrix(y_test,y_pred_1))
print('\n---------------------------------\n')

model_2 = LinearSVC()
model_2.fit(X_train_vectorized, y_train)
y_pred_2 = model_2.predict(X_test_vectorized)

print('Accuracy : ',accuracy_score(y_test,y_pred_2))
print('Classification Report\n',classification_report(y_test,y_pred_2))
print('Confusion Matrix\n',confusion_matrix(y_test,y_pred_2))