# -*- coding: utf-8 -*-
"""RESUME_MATCHING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x_yYApfcV9DsQneYEGnrA_LAmtviMbGv
"""

from google.colab import files
uploaded = files.upload()

import os
import re
import zipfile

local_zip = r'archive (5).zip'
zip_ref = zipfile.ZipFile(local_zip,'r')
zip_ref.extractall('/tmp')
zip_ref.close()

for root,dir,file in os.walk('/tmp'):
  for filename in file:
    print(os.path.join('/tmp',filename))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/tmp/job_applicant_dataset.csv')
print(df.shape)
print(df.columns)
df.head()

df.isnull().sum()

# Preprocess Resume,Job Description,Job Roles for the training these text

df['tag'] = df['Resume'] + df['Job Description']

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess(text):
  text = text.lower()
  text = re.sub(r'[^\w\s]', '', text)
  stop_words = set(stopwords.words('english'))
  words = word_tokenize(text)
  words = [word for word in words if word not in stop_words]
  lemmatizer = WordNetLemmatizer()
  words = [lemmatizer.lemmatize(word) for word in words]
  preprocessed_word = ' '.join(words)
  return preprocessed_word
df['tag'] = df['tag'].apply(preprocess)
print(df['tag'].iloc[1])

from sklearn.preprocessing import LabelEncoder,StandardScaler
le = LabelEncoder()
ss = StandardScaler()

df['Ethnicity'] = le.fit_transform(df['Ethnicity'])
df['Job Roles'] = le.fit_transform(df['Job Roles'])
df['Age'] = ss.fit_transform(df[['Age']]).flatten()

df_rec = df['Gender'].astype(str) + " " + \
          df['Ethnicity'].astype(str) + " " + \
          df['Age'].astype(str) + " " + \
          df['tag'].astype(str) + " " + \
          df['Job Roles'].astype(str)

print(df_rec.iloc[1])

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

#tfidf = TfidfVectorizer(max_features=5000)
#df_rec = tfidf.fit_transform(df_rec).toarray()

cv = CountVectorizer(max_features=5000)
df_rec = cv.fit_transform(df_rec).toarray()

X_train,X_test,y_train,y_test = train_test_split(df_rec,df['Best Match'],test_size=0.2,random_state=2)

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

model = LogisticRegression()
model.fit(X_train,y_train)
y_pred = model.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))


model_1 = XGBClassifier()
model_1.fit(X_train,y_train)
y_pred = model_1.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

model_2= RandomForestClassifier()
model_2.fit(X_train,y_train)
y_pred = model_2.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

